{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 files...\n",
      "Processed 100 files...\n",
      "Processed 200 files...\n",
      "Processed 300 files...\n",
      "Processed 400 files...\n",
      "Error processing file C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2018\\201800479: 'str' object has no attribute 'get'\n",
      "Processed 500 files...\n",
      "Processed 600 files...\n",
      "Processed 700 files...\n",
      "Processed 800 files...\n",
      "Error processing file C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2018\\201800831: 'str' object has no attribute 'get'\n",
      "Processed 900 files...\n",
      "Processed 1000 files...\n",
      "Processed 1100 files...\n",
      "Processed 1200 files...\n",
      "Processed 1300 files...\n",
      "Processed 1400 files...\n",
      "Processed 1500 files...\n",
      "Processed 1600 files...\n",
      "Error processing file C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2018\\201801609: 'str' object has no attribute 'get'\n",
      "Error processing file C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2018\\201801699: 'str' object has no attribute 'get'\n",
      "Processed 1700 files...\n",
      "Processed 1800 files...\n",
      "Error processing file C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2018\\201801828: 'str' object has no attribute 'get'\n",
      "Processed 1900 files...\n",
      "Error processing file C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2018\\201801915: 'str' object has no attribute 'get'\n",
      "Error processing file C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2018\\201801961: 'str' object has no attribute 'get'\n",
      "Processed 2000 files...\n",
      "Processed 2100 files...\n",
      "Processed 2200 files...\n",
      "Error processing file C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2018\\201802281: 'str' object has no attribute 'get'\n",
      "Processed 2300 files...\n",
      "Processed 2400 files...\n",
      "Processed 2500 files...\n",
      "Processed 2600 files...\n",
      "Processed 2700 files...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def process_json_to_df(file_path):\n",
    "    try:\n",
    "        # Read JSON file\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        abstracts_data = data.get('abstracts-retrieval-response', {})\n",
    "        coredata = abstracts_data.get('coredata', {})\n",
    "        authors_data = abstracts_data.get('authors', {}).get('author', [])\n",
    "        \n",
    "        # Helper function to get author count from affiliation.author-group\n",
    "        def get_author_count():\n",
    "            try:\n",
    "                return len(authors_data) if authors_data else 0\n",
    "            except Exception:\n",
    "                return 0\n",
    "\n",
    "        # Helper function to get open access status\n",
    "        def get_open_access():\n",
    "            try:\n",
    "                return coredata.get('openaccess', None)  # Assumes 'openaccess' exists in coredata\n",
    "            except Exception:\n",
    "                return None\n",
    "\n",
    "        # Helper function to get country from affiliation\n",
    "        def get_unique_affiliations():\n",
    "            try:\n",
    "            \n",
    "                affiliations = []\n",
    "                for author in authors_data:\n",
    "                    # Get the affiliation field\n",
    "                    # print(author)\n",
    "                    affiliation = author.get('affiliation')\n",
    "                    if isinstance(affiliation, list):\n",
    "                        for aff in affiliation:\n",
    "                            affiliations.append(aff.get(\"@id\"))\n",
    "                    elif isinstance(affiliation, dict):\n",
    "                        affiliations.append(affiliation.get(\"@id\"))\n",
    "                \n",
    "                # Remove duplicates using set and count unique affiliations\n",
    "                unique_affiliations = len(set(filter(None, affiliations)))\n",
    "                return unique_affiliations\n",
    "            except Exception as e:\n",
    "                print(f\"Error in get_unique_affiliations: {str(e)}\")\n",
    "                return 0\n",
    "        def cite_count():\n",
    "            return coredata.get('citedby-count')\n",
    "        def cover_date():\n",
    "            return coredata.get('prism:coverDate')\n",
    "        def authkeyword():\n",
    "            # print(keyword)\n",
    "            keyword = abstracts_data.get('authkeywords',{})\n",
    "            if keyword is not None:\n",
    "                keywords = [keywor.get('$') for keywor in keyword.get('author-keyword',[]) if keywor.get('$')]\n",
    "                return ', '.join(keywords)  \n",
    "            else:\n",
    "                return None\n",
    "            # print(keywords)\n",
    "            # return 0\n",
    "        # Create a dictionary with the data\n",
    "        paper_data = {\n",
    "            'Title': coredata.get('dc:title') or None,\n",
    "            'cover_date': cover_date(),\n",
    "            'authkey_word': authkeyword()\n",
    "        }\n",
    "        \n",
    "        return paper_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "        # Return empty dictionary with None values\n",
    "        return {\n",
    "            'Title': None,\n",
    "            'cover_date':None,\n",
    "            'authkey_word':None\n",
    "        }\n",
    "\n",
    "# Rest of your code remains the same\n",
    "base_path = r'C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2018\\2018'\n",
    "all_data = []\n",
    "f = []\n",
    "for (dirpath, dirnames, filenames) in walk(r'C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2018'):\n",
    "    f.extend(filenames)\n",
    "    break\n",
    "for i in range(len(f)):\n",
    "# for i in range(100):\n",
    "    file_number = str(i).zfill(5)\n",
    "    file_path = base_path + file_number\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        paper_data = process_json_to_df(file_path)\n",
    "        all_data.append(paper_data)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(f\"Processed {i} files...\")\n",
    "\n",
    "df = pd.DataFrame(all_data)\n",
    "df_1 = df.dropna()\n",
    "df_1.to_csv('2018_trend_keyword.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title           0\n",
       "cover_date      0\n",
       "authkey_word    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018\n",
      "Processed 0 files...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from os import walk\n",
    "def process_json_to_df(file_path):\n",
    "    try:\n",
    "        # Read JSON file\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        abstracts_data = data.get('abstracts-retrieval-response', {})\n",
    "        coredata = abstracts_data.get('coredata', {})\n",
    "        authors_data = abstracts_data.get('authors', {}).get('author', [])\n",
    "        subject_areas = abstracts_data.get('subject-areas', {})\n",
    "        # Helper function to get author count from affiliation.author-group\n",
    "        def get_author_count():\n",
    "            try:\n",
    "                return len(authors_data) if authors_data else 0\n",
    "            except Exception:\n",
    "                return 0\n",
    "\n",
    "        # Helper function to get open access status\n",
    "        def get_open_access():\n",
    "            try:\n",
    "                return coredata.get('openaccess', None)  # Assumes 'openaccess' exists in coredata\n",
    "            except Exception:\n",
    "                return None\n",
    "\n",
    "        # Helper function to get country from affiliation\n",
    "        def get_unique_affiliations():\n",
    "            try:\n",
    "            \n",
    "                affiliations = []\n",
    "                for author in authors_data:\n",
    "                    # Get the affiliation field\n",
    "                    # print(author)\n",
    "                    affiliation = author.get('affiliation')\n",
    "                    if isinstance(affiliation, list):\n",
    "                        for aff in affiliation:\n",
    "                            affiliations.append(aff.get(\"@id\"))\n",
    "                    elif isinstance(affiliation, dict):\n",
    "                        affiliations.append(affiliation.get(\"@id\"))\n",
    "                \n",
    "                # Remove duplicates using set and count unique affiliations\n",
    "                unique_affiliations = len(set(filter(None, affiliations)))\n",
    "                return unique_affiliations\n",
    "            except Exception as e:\n",
    "                print(f\"Error in get_unique_affiliations: {str(e)}\")\n",
    "                return 0\n",
    "        def cite_count():\n",
    "            return coredata.get('citedby-count')\n",
    "        def cover_date():\n",
    "            year = coredata.get('prism:coverDate').split('-')\n",
    "            # print(year[0])\n",
    "            return year[0]\n",
    "        def get_year():\n",
    "            return \n",
    "        def authkeyword():\n",
    "            # print(keyword)\n",
    "            keyword = abstracts_data.get('authkeywords',{})\n",
    "            if keyword is not None:\n",
    "                keywords = [keywor.get('$') for keywor in keyword.get('author-keyword',[]) if keywor.get('$')]\n",
    "                return ', '.join(keywords)  \n",
    "            else:\n",
    "                return None\n",
    "        def get_subject_area():\n",
    "            try:\n",
    "                abbrevs = [area[\"@abbrev\"] for area in subject_areas[\"subject-area\"]]\n",
    "                return \", \".join(abbrevs)\n",
    "            except Exception:\n",
    "                return 0\n",
    "            # print(keywords)\n",
    "            # return 0\n",
    "        # Create a dictionary with the data\n",
    "        paper_data = {\n",
    "            'Title': coredata.get('dc:title') or None,\n",
    "            'cover_date': cover_date(),\n",
    "            'subject_area': get_subject_area(),\n",
    "            'Year': get_year()\n",
    "        }\n",
    "        \n",
    "        return paper_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "        # Return empty dictionary with None values\n",
    "        return {\n",
    "            'Title': None,\n",
    "            'cover_date':None,\n",
    "            'authkey_word':None,\n",
    "            'Year':None\n",
    "        }\n",
    "\n",
    "# Rest of your code remains the same\n",
    "base_path = r'C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2018\\2018'\n",
    "all_data = []\n",
    "f = []\n",
    "for (dirpath, dirnames, filenames) in walk(r'C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2018'):\n",
    "    f.extend(filenames)\n",
    "    break\n",
    "# for i in range(len(f)):\n",
    "for i in range(1):  \n",
    "    file_number = str(i).zfill(5)\n",
    "    file_path = base_path + file_number\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        paper_data = process_json_to_df(file_path)\n",
    "        all_data.append(paper_data)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(f\"Processed {i} files...\")\n",
    "\n",
    "df = pd.DataFrame(all_data)\n",
    "# df\n",
    "# # df_1 = df.dropna()\n",
    "# df.to_csv('2023_trend_topic.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "\n",
    "# spark_home = r'C:\\Users\\USER\\Downloads\\spark-3.5.3\\spark-3.5.3-bin-hadoop3'\n",
    "# os.environ['SPARK_HOME'] = spark_home\n",
    "# sys.path.append(os.path.join(spark_home, 'python'))\n",
    "# sys.path.append(os.path.join(spark_home, 'python', 'lib', 'py4j-0.10.9.7-src.zip'))  # Replace with actual py4j version\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "# spark_url = 'local'\n",
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql import SQLContext\n",
    "\n",
    "# spark = SparkSession.builder\\\n",
    "#         .master(spark_url)\\\n",
    "#         .appName('Spark Tutorial')\\\n",
    "#         .config('spark.ui.port', '4040')\\\n",
    "#         .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Public health and international epidemiology f...</td>\n",
       "      <td>Pongpirul K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Public health and international epidemiology f...</td>\n",
       "      <td>Lungren M.P.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Flexible Printed Active Antenna for Digital Te...</td>\n",
       "      <td>Pratumsiri T.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Flexible Printed Active Antenna for Digital Te...</td>\n",
       "      <td>Janpugdee P.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Parametric study of hydrogen production via so...</td>\n",
       "      <td>Phuakpunk K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349215</th>\n",
       "      <td>Bullying at work: Cognitive appraisal of negat...</td>\n",
       "      <td>Visockaite G.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349216</th>\n",
       "      <td>Bullying at work: Cognitive appraisal of negat...</td>\n",
       "      <td>Roongrerngsuke S.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349217</th>\n",
       "      <td>Three-dimensional interaction diagram for the ...</td>\n",
       "      <td>Keawsawasvong S.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349218</th>\n",
       "      <td>Three-dimensional interaction diagram for the ...</td>\n",
       "      <td>Ukritchon B.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349219</th>\n",
       "      <td>Changing patterns of civil-military relations ...</td>\n",
       "      <td>Bamrungsuk S.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>349220 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Title             Author\n",
       "0       Public health and international epidemiology f...       Pongpirul K.\n",
       "1       Public health and international epidemiology f...       Lungren M.P.\n",
       "2       Flexible Printed Active Antenna for Digital Te...      Pratumsiri T.\n",
       "3       Flexible Printed Active Antenna for Digital Te...       Janpugdee P.\n",
       "4       Parametric study of hydrogen production via so...       Phuakpunk K.\n",
       "...                                                   ...                ...\n",
       "349215  Bullying at work: Cognitive appraisal of negat...      Visockaite G.\n",
       "349216  Bullying at work: Cognitive appraisal of negat...  Roongrerngsuke S.\n",
       "349217  Three-dimensional interaction diagram for the ...   Keawsawasvong S.\n",
       "349218  Three-dimensional interaction diagram for the ...       Ukritchon B.\n",
       "349219  Changing patterns of civil-military relations ...      Bamrungsuk S.\n",
       "\n",
       "[349220 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from os import walk\n",
    "\n",
    "def process_json_to_df(file_path):\n",
    "    try:\n",
    "        # Read JSON file\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        abstracts_data = data.get('abstracts-retrieval-response', {})\n",
    "        coredata = abstracts_data.get('coredata', {})\n",
    "        authors_data = abstracts_data.get('authors', {}).get('author', [])\n",
    "        \n",
    "        # Extract title\n",
    "        title = coredata.get('dc:title') or None\n",
    "        # print(title)\n",
    "        # Create a list of dictionaries for each author with the title\n",
    "        paper_data = []\n",
    "        for author in authors_data:\n",
    "            author_name = author.get('preferred-name', {}).get('ce:indexed-name')\n",
    "            # print(author_name)\n",
    "            paper_data.append({'Title': title, 'Author': author_name})\n",
    "        \n",
    "        return paper_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Main processing loop\n",
    "base_path = r'C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2018\\2018'\n",
    "all_data = []\n",
    "f = []\n",
    "for (dirpath, dirnames, filenames) in walk(r'C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2018'):\n",
    "    f.extend(filenames)\n",
    "    break\n",
    "for i in range(len(f)):\n",
    "# for i in range(2):  \n",
    "    file_number = str(i).zfill(5)\n",
    "    file_path = base_path + file_number\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        paper_data = process_json_to_df(file_path)\n",
    "        all_data.extend(paper_data)\n",
    "    \n",
    "    \n",
    "    # print(f\"Processed {i} files...\")\n",
    "\n",
    "df = pd.DataFrame(all_data)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('author_for_network_analysis.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from os import walk\n",
    "\n",
    "def process_json_to_df(file_path):\n",
    "    try:\n",
    "        # Read JSON file\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        abstracts_data = data.get('abstracts-retrieval-response', {})\n",
    "        coredata = abstracts_data.get('coredata', {})\n",
    "        authors_data = abstracts_data.get('authors', {}).get('author', [])\n",
    "        \n",
    "        # Extract title\n",
    "        title = coredata.get('dc:title') or None\n",
    "        # print(title)\n",
    "        # Create a list of dictionaries for each author with the title\n",
    "        paper_data = []\n",
    "        for author in authors_data:\n",
    "            author_name = author.get('preferred-name', {}).get('ce:indexed-name')\n",
    "            # print(author_name)\n",
    "            paper_data.append({'Title': title, 'Author': author_name})\n",
    "        \n",
    "        return paper_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Main processing loop\n",
    "base_path = r'C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2018\\2018'\n",
    "all_data = []\n",
    "f = []\n",
    "for (dirpath, dirnames, filenames) in walk(r'C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2018'):\n",
    "    f.extend(filenames)\n",
    "    break\n",
    "for i in range(len(f)):\n",
    "# for i in range(2):  \n",
    "    file_number = str(i).zfill(5)\n",
    "    file_path = base_path + file_number\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        paper_data = process_json_to_df(file_path)\n",
    "        all_data.extend(paper_data)\n",
    "    \n",
    "    \n",
    "    # print(f\"Processed {i} files...\")\n",
    "\n",
    "df = pd.DataFrame(all_data)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 files...\n",
      "Processed 100 files...\n",
      "Processed 200 files...\n",
      "Processed 300 files...\n",
      "Processed 400 files...\n",
      "Processed 500 files...\n",
      "Processed 600 files...\n",
      "Processed 700 files...\n",
      "Processed 800 files...\n",
      "Processed 900 files...\n",
      "Processed 1000 files...\n",
      "Processed 1100 files...\n",
      "Processed 1200 files...\n",
      "Processed 1300 files...\n",
      "Processed 1400 files...\n",
      "Processed 1500 files...\n",
      "Processed 1600 files...\n",
      "Processed 1700 files...\n",
      "Processed 1800 files...\n",
      "Processed 1900 files...\n",
      "Processed 2000 files...\n",
      "Processed 2100 files...\n",
      "Processed 2200 files...\n",
      "Processed 2300 files...\n",
      "Processed 2400 files...\n",
      "Error processing file C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2023\\202302422: 'list' object has no attribute 'get'\n",
      "Processed 2500 files...\n",
      "Processed 2600 files...\n",
      "Processed 2700 files...\n",
      "Processed 2800 files...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>affliation_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Graphene oxide-alginate hydrogel-based indicat...</td>\n",
       "      <td>Thailand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rare coordination behavior of triethanolamine ...</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Total ammonia nitrogen removal and microbial c...</td>\n",
       "      <td>Thailand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Effects of microaeration and sludge recirculat...</td>\n",
       "      <td>Thailand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bioaccumulation of heavy metals in commerciall...</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2885</th>\n",
       "      <td>Long-chain bio-olefins production via oxidativ...</td>\n",
       "      <td>Thailand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2886</th>\n",
       "      <td>Recent Developments and Applications of Microf...</td>\n",
       "      <td>Thailand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2887</th>\n",
       "      <td>Social justice, education and peacebuilding: c...</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2888</th>\n",
       "      <td>Effects of black soldier fly (Hermetia illucen...</td>\n",
       "      <td>Thailand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2889</th>\n",
       "      <td>Effects of remittances on household poverty an...</td>\n",
       "      <td>Cambodia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2890 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Title affliation_country\n",
       "0     Graphene oxide-alginate hydrogel-based indicat...           Thailand\n",
       "1     Rare coordination behavior of triethanolamine ...              India\n",
       "2     Total ammonia nitrogen removal and microbial c...           Thailand\n",
       "3     Effects of microaeration and sludge recirculat...           Thailand\n",
       "4     Bioaccumulation of heavy metals in commerciall...              India\n",
       "...                                                 ...                ...\n",
       "2885  Long-chain bio-olefins production via oxidativ...           Thailand\n",
       "2886  Recent Developments and Applications of Microf...           Thailand\n",
       "2887  Social justice, education and peacebuilding: c...     United Kingdom\n",
       "2888  Effects of black soldier fly (Hermetia illucen...           Thailand\n",
       "2889  Effects of remittances on household poverty an...           Cambodia\n",
       "\n",
       "[2890 rows x 2 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "def process_json_to_df(file_path):\n",
    "    try:\n",
    "        # Read JSON file\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        abstracts_data = data.get('abstracts-retrieval-response', {})\n",
    "        coredata = abstracts_data.get('coredata', {})\n",
    "        authors_data = abstracts_data.get('authors', {}).get('author', [])\n",
    "        \n",
    "        # Helper function to get author count from affiliation.author-group\n",
    "        def get_author_count():\n",
    "            try:\n",
    "                return len(authors_data) if authors_data else 0\n",
    "            except Exception:\n",
    "                return 0\n",
    "\n",
    "        # Helper function to get open access status\n",
    "        def get_open_access():\n",
    "            try:\n",
    "                return coredata.get('openaccess', None)  # Assumes 'openaccess' exists in coredata\n",
    "            except Exception:\n",
    "                return None\n",
    "        def get_country():\n",
    "            try:\n",
    "                author_group = abstracts_data.get('item', {}).get('bibrecord', {}).get('head', {}).get('author-group', [])\n",
    "\n",
    "                return author_group.get('affiliation',{}).get('country',{})\n",
    "            except Exception:\n",
    "                return author_group[0].get('affiliation',[]).get('country',{})\n",
    "        # Helper function to get country from affiliation\n",
    "        def get_unique_affiliations():\n",
    "            try:\n",
    "            \n",
    "                affiliations = []\n",
    "                for author in authors_data:\n",
    "                    # Get the affiliation field\n",
    "                    # print(author)\n",
    "                    affiliation = author.get('affiliation')\n",
    "                    if isinstance(affiliation, list):\n",
    "                        for aff in affiliation:\n",
    "                            affiliations.append(aff.get(\"@id\"))\n",
    "                    elif isinstance(affiliation, dict):\n",
    "                        affiliations.append(affiliation.get(\"@id\"))\n",
    "                \n",
    "                # Remove duplicates using set and count unique affiliations\n",
    "                unique_affiliations = len(set(filter(None, affiliations)))\n",
    "                return unique_affiliations\n",
    "            except Exception as e:\n",
    "                print(f\"Error in get_unique_affiliations: {str(e)}\")\n",
    "                return 0\n",
    "        def cite_count():\n",
    "            return coredata.get('citedby-count')\n",
    "        def cover_date():\n",
    "            return coredata.get('prism:coverDate')\n",
    "        def authkeyword():\n",
    "            # print(keyword)\n",
    "            keyword = abstracts_data.get('authkeywords',{})\n",
    "            if keyword is not None:\n",
    "                keywords = [keywor.get('$') for keywor in keyword.get('author-keyword',[]) if keywor.get('$')]\n",
    "                return ', '.join(keywords)  \n",
    "            else:\n",
    "                return None\n",
    "            # print(keywords)\n",
    "            # return 0\n",
    "        # Create a dictionary with the data\n",
    "        paper_data = {\n",
    "            'Title': coredata.get('dc:title') or None,\n",
    "            'affliation_country': get_country()\n",
    "            # 'cover_date': cover_date(),\n",
    "            # 'authkey_word': authkeyword()\n",
    "        }\n",
    "        \n",
    "        return paper_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "        # Return empty dictionary with None values\n",
    "        return {\n",
    "            'Title': None,\n",
    "            'affliation_country':None\n",
    "        }\n",
    "\n",
    "# Rest of your code remains the same\n",
    "base_path = r'C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2023\\2023'\n",
    "all_data = []\n",
    "f = []\n",
    "for (dirpath, dirnames, filenames) in walk(r'C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2023'):\n",
    "    f.extend(filenames)\n",
    "    break\n",
    "for i in range(len(f)):\n",
    "# for i in range(2):\n",
    "    file_number = str(i).zfill(5)\n",
    "    file_path = base_path + file_number\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        paper_data = process_json_to_df(file_path)\n",
    "        all_data.append(paper_data)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(f\"Processed {i} files...\")\n",
    "\n",
    "df = pd.DataFrame(all_data)\n",
    "# df_1 = df.dropna()\n",
    "# df_1.to_csv('2018_trend_keyword.csv',index = False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['affliation_country'].value_counts()\n",
    "df.to_csv('2023_country.csv',index= False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_chula_datasci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
