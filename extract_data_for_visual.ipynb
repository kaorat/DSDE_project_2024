{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 files...\n",
      "Processed 100 files...\n",
      "Processed 200 files...\n",
      "Processed 300 files...\n",
      "Processed 400 files...\n",
      "Error processing file C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2018\\201800479: 'str' object has no attribute 'get'\n",
      "Processed 500 files...\n",
      "Processed 600 files...\n",
      "Processed 700 files...\n",
      "Processed 800 files...\n",
      "Error processing file C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2018\\201800831: 'str' object has no attribute 'get'\n",
      "Processed 900 files...\n",
      "Processed 1000 files...\n",
      "Processed 1100 files...\n",
      "Processed 1200 files...\n",
      "Processed 1300 files...\n",
      "Processed 1400 files...\n",
      "Processed 1500 files...\n",
      "Processed 1600 files...\n",
      "Error processing file C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2018\\201801609: 'str' object has no attribute 'get'\n",
      "Error processing file C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2018\\201801699: 'str' object has no attribute 'get'\n",
      "Processed 1700 files...\n",
      "Processed 1800 files...\n",
      "Error processing file C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2018\\201801828: 'str' object has no attribute 'get'\n",
      "Processed 1900 files...\n",
      "Error processing file C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2018\\201801915: 'str' object has no attribute 'get'\n",
      "Error processing file C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2018\\201801961: 'str' object has no attribute 'get'\n",
      "Processed 2000 files...\n",
      "Processed 2100 files...\n",
      "Processed 2200 files...\n",
      "Error processing file C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2018\\201802281: 'str' object has no attribute 'get'\n",
      "Processed 2300 files...\n",
      "Processed 2400 files...\n",
      "Processed 2500 files...\n",
      "Processed 2600 files...\n",
      "Processed 2700 files...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def process_json_to_df(file_path):\n",
    "    try:\n",
    "        # Read JSON file\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        abstracts_data = data.get('abstracts-retrieval-response', {})\n",
    "        coredata = abstracts_data.get('coredata', {})\n",
    "        authors_data = abstracts_data.get('authors', {}).get('author', [])\n",
    "        \n",
    "        # Helper function to get author count from affiliation.author-group\n",
    "        def get_author_count():\n",
    "            try:\n",
    "                return len(authors_data) if authors_data else 0\n",
    "            except Exception:\n",
    "                return 0\n",
    "\n",
    "        # Helper function to get open access status\n",
    "        def get_open_access():\n",
    "            try:\n",
    "                return coredata.get('openaccess', None)  # Assumes 'openaccess' exists in coredata\n",
    "            except Exception:\n",
    "                return None\n",
    "\n",
    "        # Helper function to get country from affiliation\n",
    "        def get_unique_affiliations():\n",
    "            try:\n",
    "            \n",
    "                affiliations = []\n",
    "                for author in authors_data:\n",
    "                    # Get the affiliation field\n",
    "                    # print(author)\n",
    "                    affiliation = author.get('affiliation')\n",
    "                    if isinstance(affiliation, list):\n",
    "                        for aff in affiliation:\n",
    "                            affiliations.append(aff.get(\"@id\"))\n",
    "                    elif isinstance(affiliation, dict):\n",
    "                        affiliations.append(affiliation.get(\"@id\"))\n",
    "                \n",
    "                # Remove duplicates using set and count unique affiliations\n",
    "                unique_affiliations = len(set(filter(None, affiliations)))\n",
    "                return unique_affiliations\n",
    "            except Exception as e:\n",
    "                print(f\"Error in get_unique_affiliations: {str(e)}\")\n",
    "                return 0\n",
    "        def cite_count():\n",
    "            return coredata.get('citedby-count')\n",
    "        def cover_date():\n",
    "            return coredata.get('prism:coverDate')\n",
    "        def authkeyword():\n",
    "            # print(keyword)\n",
    "            keyword = abstracts_data.get('authkeywords',{})\n",
    "            if keyword is not None:\n",
    "                keywords = [keywor.get('$') for keywor in keyword.get('author-keyword',[]) if keywor.get('$')]\n",
    "                return ', '.join(keywords)  \n",
    "            else:\n",
    "                return None\n",
    "            # print(keywords)\n",
    "            # return 0\n",
    "        # Create a dictionary with the data\n",
    "        paper_data = {\n",
    "            'Title': coredata.get('dc:title') or None,\n",
    "            'cover_date': cover_date(),\n",
    "            'authkey_word': authkeyword()\n",
    "        }\n",
    "        \n",
    "        return paper_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "        # Return empty dictionary with None values\n",
    "        return {\n",
    "            'Title': None,\n",
    "            'cover_date':None,\n",
    "            'authkey_word':None\n",
    "        }\n",
    "\n",
    "# Rest of your code remains the same\n",
    "base_path = r'C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2018\\2018'\n",
    "all_data = []\n",
    "f = []\n",
    "for (dirpath, dirnames, filenames) in walk(r'C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2018'):\n",
    "    f.extend(filenames)\n",
    "    break\n",
    "for i in range(len(f)):\n",
    "# for i in range(100):\n",
    "    file_number = str(i).zfill(5)\n",
    "    file_path = base_path + file_number\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        paper_data = process_json_to_df(file_path)\n",
    "        all_data.append(paper_data)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(f\"Processed {i} files...\")\n",
    "\n",
    "df = pd.DataFrame(all_data)\n",
    "df_1 = df.dropna()\n",
    "df_1.to_csv('2018_trend_keyword.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title           0\n",
       "cover_date      0\n",
       "authkey_word    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 files...\n",
      "Processed 100 files...\n",
      "Processed 200 files...\n",
      "Processed 300 files...\n",
      "Processed 400 files...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 98\u001b[0m\n\u001b[0;32m     95\u001b[0m file_path \u001b[38;5;241m=\u001b[39m base_path \u001b[38;5;241m+\u001b[39m file_number\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(file_path):\n\u001b[1;32m---> 98\u001b[0m     paper_data \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_json_to_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m     all_data\u001b[38;5;241m.\u001b[39mappend(paper_data)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[37], line 5\u001b[0m, in \u001b[0;36mprocess_json_to_df\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_json_to_df\u001b[39m(file_path):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;66;03m# Read JSON file\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      6\u001b[0m             data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[0;32m      8\u001b[0m         abstracts_data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabstracts-retrieval-response\u001b[39m\u001b[38;5;124m'\u001b[39m, {})\n",
      "File \u001b[1;32mc:\\Users\\USER\\miniconda3\\envs\\env_chula_datasci\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen codecs>:309\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, errors)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "def process_json_to_df(file_path):\n",
    "    try:\n",
    "        # Read JSON file\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        abstracts_data = data.get('abstracts-retrieval-response', {})\n",
    "        coredata = abstracts_data.get('coredata', {})\n",
    "        authors_data = abstracts_data.get('authors', {}).get('author', [])\n",
    "        subject_areas = abstracts_data.get('subject-areas', {})\n",
    "        # Helper function to get author count from affiliation.author-group\n",
    "        def get_author_count():\n",
    "            try:\n",
    "                return len(authors_data) if authors_data else 0\n",
    "            except Exception:\n",
    "                return 0\n",
    "\n",
    "        # Helper function to get open access status\n",
    "        def get_open_access():\n",
    "            try:\n",
    "                return coredata.get('openaccess', None)  # Assumes 'openaccess' exists in coredata\n",
    "            except Exception:\n",
    "                return None\n",
    "\n",
    "        # Helper function to get country from affiliation\n",
    "        def get_unique_affiliations():\n",
    "            try:\n",
    "            \n",
    "                affiliations = []\n",
    "                for author in authors_data:\n",
    "                    # Get the affiliation field\n",
    "                    # print(author)\n",
    "                    affiliation = author.get('affiliation')\n",
    "                    if isinstance(affiliation, list):\n",
    "                        for aff in affiliation:\n",
    "                            affiliations.append(aff.get(\"@id\"))\n",
    "                    elif isinstance(affiliation, dict):\n",
    "                        affiliations.append(affiliation.get(\"@id\"))\n",
    "                \n",
    "                # Remove duplicates using set and count unique affiliations\n",
    "                unique_affiliations = len(set(filter(None, affiliations)))\n",
    "                return unique_affiliations\n",
    "            except Exception as e:\n",
    "                print(f\"Error in get_unique_affiliations: {str(e)}\")\n",
    "                return 0\n",
    "        def cite_count():\n",
    "            return coredata.get('citedby-count')\n",
    "        def cover_date():\n",
    "            return coredata.get('prism:coverDate')\n",
    "        def authkeyword():\n",
    "            # print(keyword)\n",
    "            keyword = abstracts_data.get('authkeywords',{})\n",
    "            if keyword is not None:\n",
    "                keywords = [keywor.get('$') for keywor in keyword.get('author-keyword',[]) if keywor.get('$')]\n",
    "                return ', '.join(keywords)  \n",
    "            else:\n",
    "                return None\n",
    "        def get_subject_area():\n",
    "            try:\n",
    "                abbrevs = [area[\"@abbrev\"] for area in subject_areas[\"subject-area\"]]\n",
    "                return \", \".join(abbrevs)\n",
    "            except Exception:\n",
    "                return 0\n",
    "            # print(keywords)\n",
    "            # return 0\n",
    "        # Create a dictionary with the data\n",
    "        paper_data = {\n",
    "            'Title': coredata.get('dc:title') or None,\n",
    "            'cover_date': cover_date(),\n",
    "            'subject_area': get_subject_area()\n",
    "        }\n",
    "        \n",
    "        return paper_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "        # Return empty dictionary with None values\n",
    "        return {\n",
    "            'Title': None,\n",
    "            'cover_date':None,\n",
    "            'authkey_word':None\n",
    "        }\n",
    "\n",
    "# Rest of your code remains the same\n",
    "base_path = r'C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2023\\2023'\n",
    "all_data = []\n",
    "f = []\n",
    "for (dirpath, dirnames, filenames) in walk(r'C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2023'):\n",
    "    f.extend(filenames)\n",
    "    break\n",
    "for i in range(len(f)):\n",
    "# for i in range(100):  \n",
    "    file_number = str(i).zfill(5)\n",
    "    file_path = base_path + file_number\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        paper_data = process_json_to_df(file_path)\n",
    "        all_data.append(paper_data)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(f\"Processed {i} files...\")\n",
    "\n",
    "df = pd.DataFrame(all_data)\n",
    "# df\n",
    "# # df_1 = df.dropna()\n",
    "# df.to_csv('2023_trend_topic.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "\n",
    "# spark_home = r'C:\\Users\\USER\\Downloads\\spark-3.5.3\\spark-3.5.3-bin-hadoop3'\n",
    "# os.environ['SPARK_HOME'] = spark_home\n",
    "# sys.path.append(os.path.join(spark_home, 'python'))\n",
    "# sys.path.append(os.path.join(spark_home, 'python', 'lib', 'py4j-0.10.9.7-src.zip'))  # Replace with actual py4j version\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "# spark_url = 'local'\n",
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql import SQLContext\n",
    "\n",
    "# spark = SparkSession.builder\\\n",
    "#         .master(spark_url)\\\n",
    "#         .appName('Spark Tutorial')\\\n",
    "#         .config('spark.ui.port', '4040')\\\n",
    "#         .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Public health and international epidemiology f...</td>\n",
       "      <td>Pongpirul K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Public health and international epidemiology f...</td>\n",
       "      <td>Lungren M.P.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Flexible Printed Active Antenna for Digital Te...</td>\n",
       "      <td>Pratumsiri T.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Flexible Printed Active Antenna for Digital Te...</td>\n",
       "      <td>Janpugdee P.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Parametric study of hydrogen production via so...</td>\n",
       "      <td>Phuakpunk K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349215</th>\n",
       "      <td>Bullying at work: Cognitive appraisal of negat...</td>\n",
       "      <td>Visockaite G.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349216</th>\n",
       "      <td>Bullying at work: Cognitive appraisal of negat...</td>\n",
       "      <td>Roongrerngsuke S.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349217</th>\n",
       "      <td>Three-dimensional interaction diagram for the ...</td>\n",
       "      <td>Keawsawasvong S.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349218</th>\n",
       "      <td>Three-dimensional interaction diagram for the ...</td>\n",
       "      <td>Ukritchon B.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349219</th>\n",
       "      <td>Changing patterns of civil-military relations ...</td>\n",
       "      <td>Bamrungsuk S.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>349220 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Title             Author\n",
       "0       Public health and international epidemiology f...       Pongpirul K.\n",
       "1       Public health and international epidemiology f...       Lungren M.P.\n",
       "2       Flexible Printed Active Antenna for Digital Te...      Pratumsiri T.\n",
       "3       Flexible Printed Active Antenna for Digital Te...       Janpugdee P.\n",
       "4       Parametric study of hydrogen production via so...       Phuakpunk K.\n",
       "...                                                   ...                ...\n",
       "349215  Bullying at work: Cognitive appraisal of negat...      Visockaite G.\n",
       "349216  Bullying at work: Cognitive appraisal of negat...  Roongrerngsuke S.\n",
       "349217  Three-dimensional interaction diagram for the ...   Keawsawasvong S.\n",
       "349218  Three-dimensional interaction diagram for the ...       Ukritchon B.\n",
       "349219  Changing patterns of civil-military relations ...      Bamrungsuk S.\n",
       "\n",
       "[349220 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from os import walk\n",
    "\n",
    "def process_json_to_df(file_path):\n",
    "    try:\n",
    "        # Read JSON file\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        abstracts_data = data.get('abstracts-retrieval-response', {})\n",
    "        coredata = abstracts_data.get('coredata', {})\n",
    "        authors_data = abstracts_data.get('authors', {}).get('author', [])\n",
    "        \n",
    "        # Extract title\n",
    "        title = coredata.get('dc:title') or None\n",
    "        # print(title)\n",
    "        # Create a list of dictionaries for each author with the title\n",
    "        paper_data = []\n",
    "        for author in authors_data:\n",
    "            author_name = author.get('preferred-name', {}).get('ce:indexed-name')\n",
    "            # print(author_name)\n",
    "            paper_data.append({'Title': title, 'Author': author_name})\n",
    "        \n",
    "        return paper_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Main processing loop\n",
    "base_path = r'C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2018\\2018'\n",
    "all_data = []\n",
    "f = []\n",
    "for (dirpath, dirnames, filenames) in walk(r'C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2018'):\n",
    "    f.extend(filenames)\n",
    "    break\n",
    "for i in range(len(f)):\n",
    "# for i in range(2):  \n",
    "    file_number = str(i).zfill(5)\n",
    "    file_path = base_path + file_number\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        paper_data = process_json_to_df(file_path)\n",
    "        all_data.extend(paper_data)\n",
    "    \n",
    "    \n",
    "    # print(f\"Processed {i} files...\")\n",
    "\n",
    "df = pd.DataFrame(all_data)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('author_for_network_analysis.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_chula_datasci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
