{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 files...\n",
      "Processed 100 files...\n",
      "Processed 200 files...\n",
      "Processed 300 files...\n",
      "Processed 400 files...\n",
      "Error processing file C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2018\\201800479: 'str' object has no attribute 'get'\n",
      "Processed 500 files...\n",
      "Processed 600 files...\n",
      "Processed 700 files...\n",
      "Processed 800 files...\n",
      "Error processing file C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2018\\201800831: 'str' object has no attribute 'get'\n",
      "Processed 900 files...\n",
      "Processed 1000 files...\n",
      "Processed 1100 files...\n",
      "Processed 1200 files...\n",
      "Processed 1300 files...\n",
      "Processed 1400 files...\n",
      "Processed 1500 files...\n",
      "Processed 1600 files...\n",
      "Error processing file C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2018\\201801609: 'str' object has no attribute 'get'\n",
      "Error processing file C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2018\\201801699: 'str' object has no attribute 'get'\n",
      "Processed 1700 files...\n",
      "Processed 1800 files...\n",
      "Error processing file C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2018\\201801828: 'str' object has no attribute 'get'\n",
      "Processed 1900 files...\n",
      "Error processing file C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2018\\201801915: 'str' object has no attribute 'get'\n",
      "Error processing file C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2018\\201801961: 'str' object has no attribute 'get'\n",
      "Processed 2000 files...\n",
      "Processed 2100 files...\n",
      "Processed 2200 files...\n",
      "Error processing file C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2018\\201802281: 'str' object has no attribute 'get'\n",
      "Processed 2300 files...\n",
      "Processed 2400 files...\n",
      "Processed 2500 files...\n",
      "Processed 2600 files...\n",
      "Processed 2700 files...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def process_json_to_df(file_path):\n",
    "    try:\n",
    "        # Read JSON file\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        abstracts_data = data.get('abstracts-retrieval-response', {})\n",
    "        coredata = abstracts_data.get('coredata', {})\n",
    "        authors_data = abstracts_data.get('authors', {}).get('author', [])\n",
    "        \n",
    "        # Helper function to get author count from affiliation.author-group\n",
    "        def get_author_count():\n",
    "            try:\n",
    "                return len(authors_data) if authors_data else 0\n",
    "            except Exception:\n",
    "                return 0\n",
    "\n",
    "        # Helper function to get open access status\n",
    "        def get_open_access():\n",
    "            try:\n",
    "                return coredata.get('openaccess', None)  # Assumes 'openaccess' exists in coredata\n",
    "            except Exception:\n",
    "                return None\n",
    "\n",
    "        # Helper function to get country from affiliation\n",
    "        def get_unique_affiliations():\n",
    "            try:\n",
    "            \n",
    "                affiliations = []\n",
    "                for author in authors_data:\n",
    "                    # Get the affiliation field\n",
    "                    # print(author)\n",
    "                    affiliation = author.get('affiliation')\n",
    "                    if isinstance(affiliation, list):\n",
    "                        for aff in affiliation:\n",
    "                            affiliations.append(aff.get(\"@id\"))\n",
    "                    elif isinstance(affiliation, dict):\n",
    "                        affiliations.append(affiliation.get(\"@id\"))\n",
    "                \n",
    "                # Remove duplicates using set and count unique affiliations\n",
    "                unique_affiliations = len(set(filter(None, affiliations)))\n",
    "                return unique_affiliations\n",
    "            except Exception as e:\n",
    "                print(f\"Error in get_unique_affiliations: {str(e)}\")\n",
    "                return 0\n",
    "        def cite_count():\n",
    "            return coredata.get('citedby-count')\n",
    "        def cover_date():\n",
    "            return coredata.get('prism:coverDate')\n",
    "        def authkeyword():\n",
    "            # print(keyword)\n",
    "            keyword = abstracts_data.get('authkeywords',{})\n",
    "            if keyword is not None:\n",
    "                keywords = [keywor.get('$') for keywor in keyword.get('author-keyword',[]) if keywor.get('$')]\n",
    "                return ', '.join(keywords)  \n",
    "            else:\n",
    "                return None\n",
    "            # print(keywords)\n",
    "            # return 0\n",
    "        # Create a dictionary with the data\n",
    "        paper_data = {\n",
    "            'Title': coredata.get('dc:title') or None,\n",
    "            'cover_date': cover_date(),\n",
    "            'authkey_word': authkeyword()\n",
    "        }\n",
    "        \n",
    "        return paper_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "        # Return empty dictionary with None values\n",
    "        return {\n",
    "            'Title': None,\n",
    "            'cover_date':None,\n",
    "            'authkey_word':None\n",
    "        }\n",
    "\n",
    "# Rest of your code remains the same\n",
    "base_path = r'C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2018\\2018'\n",
    "all_data = []\n",
    "f = []\n",
    "for (dirpath, dirnames, filenames) in walk(r'C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2018'):\n",
    "    f.extend(filenames)\n",
    "    break\n",
    "for i in range(len(f)):\n",
    "# for i in range(100):\n",
    "    file_number = str(i).zfill(5)\n",
    "    file_path = base_path + file_number\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        paper_data = process_json_to_df(file_path)\n",
    "        all_data.append(paper_data)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(f\"Processed {i} files...\")\n",
    "\n",
    "df = pd.DataFrame(all_data)\n",
    "df_1 = df.dropna()\n",
    "df_1.to_csv('2018_trend_keyword.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title           0\n",
       "cover_date      0\n",
       "authkey_word    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 files...\n",
      "Processed 100 files...\n",
      "Processed 200 files...\n",
      "Processed 300 files...\n",
      "Processed 400 files...\n",
      "Processed 500 files...\n",
      "Processed 600 files...\n",
      "Processed 700 files...\n",
      "Processed 800 files...\n",
      "Processed 900 files...\n",
      "Processed 1000 files...\n",
      "Processed 1100 files...\n",
      "Processed 1200 files...\n",
      "Processed 1300 files...\n",
      "Processed 1400 files...\n",
      "Processed 1500 files...\n",
      "Processed 1600 files...\n",
      "Processed 1700 files...\n",
      "Processed 1800 files...\n",
      "Processed 1900 files...\n",
      "Processed 2000 files...\n",
      "Processed 2100 files...\n",
      "Processed 2200 files...\n",
      "Processed 2300 files...\n",
      "Processed 2400 files...\n",
      "Processed 2500 files...\n",
      "Processed 2600 files...\n",
      "Processed 2700 files...\n",
      "Processed 2800 files...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def process_json_to_df(file_path):\n",
    "    try:\n",
    "        # Read JSON file\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        abstracts_data = data.get('abstracts-retrieval-response', {})\n",
    "        coredata = abstracts_data.get('coredata', {})\n",
    "        authors_data = abstracts_data.get('authors', {}).get('author', [])\n",
    "        subject_areas = abstracts_data.get('subject-areas', {})\n",
    "        # Helper function to get author count from affiliation.author-group\n",
    "        def get_author_count():\n",
    "            try:\n",
    "                return len(authors_data) if authors_data else 0\n",
    "            except Exception:\n",
    "                return 0\n",
    "\n",
    "        # Helper function to get open access status\n",
    "        def get_open_access():\n",
    "            try:\n",
    "                return coredata.get('openaccess', None)  # Assumes 'openaccess' exists in coredata\n",
    "            except Exception:\n",
    "                return None\n",
    "\n",
    "        # Helper function to get country from affiliation\n",
    "        def get_unique_affiliations():\n",
    "            try:\n",
    "            \n",
    "                affiliations = []\n",
    "                for author in authors_data:\n",
    "                    # Get the affiliation field\n",
    "                    # print(author)\n",
    "                    affiliation = author.get('affiliation')\n",
    "                    if isinstance(affiliation, list):\n",
    "                        for aff in affiliation:\n",
    "                            affiliations.append(aff.get(\"@id\"))\n",
    "                    elif isinstance(affiliation, dict):\n",
    "                        affiliations.append(affiliation.get(\"@id\"))\n",
    "                \n",
    "                # Remove duplicates using set and count unique affiliations\n",
    "                unique_affiliations = len(set(filter(None, affiliations)))\n",
    "                return unique_affiliations\n",
    "            except Exception as e:\n",
    "                print(f\"Error in get_unique_affiliations: {str(e)}\")\n",
    "                return 0\n",
    "        def cite_count():\n",
    "            return coredata.get('citedby-count')\n",
    "        def cover_date():\n",
    "            return coredata.get('prism:coverDate')\n",
    "        def authkeyword():\n",
    "            # print(keyword)\n",
    "            keyword = abstracts_data.get('authkeywords',{})\n",
    "            if keyword is not None:\n",
    "                keywords = [keywor.get('$') for keywor in keyword.get('author-keyword',[]) if keywor.get('$')]\n",
    "                return ', '.join(keywords)  \n",
    "            else:\n",
    "                return None\n",
    "        def get_subject_area():\n",
    "            try:\n",
    "                abbrevs = [area[\"@abbrev\"] for area in subject_areas[\"subject-area\"]]\n",
    "                return \", \".join(abbrevs)\n",
    "            except Exception:\n",
    "                return 0\n",
    "            # print(keywords)\n",
    "            # return 0\n",
    "        # Create a dictionary with the data\n",
    "        paper_data = {\n",
    "            'Title': coredata.get('dc:title') or None,\n",
    "            'cover_date': cover_date(),\n",
    "            'subject_area': get_subject_area()\n",
    "        }\n",
    "        \n",
    "        return paper_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "        # Return empty dictionary with None values\n",
    "        return {\n",
    "            'Title': None,\n",
    "            'cover_date':None,\n",
    "            'authkey_word':None\n",
    "        }\n",
    "\n",
    "# Rest of your code remains the same\n",
    "base_path = r'C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2023\\2023'\n",
    "all_data = []\n",
    "f = []\n",
    "for (dirpath, dirnames, filenames) in walk(r'C:\\Users\\USER\\Desktop\\DSDE_project\\Project\\2023'):\n",
    "    f.extend(filenames)\n",
    "    break\n",
    "for i in range(len(f)):\n",
    "# for i in range(100):  \n",
    "    file_number = str(i).zfill(5)\n",
    "    file_path = base_path + file_number\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        paper_data = process_json_to_df(file_path)\n",
    "        all_data.append(paper_data)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(f\"Processed {i} files...\")\n",
    "\n",
    "df = pd.DataFrame(all_data)\n",
    "# df\n",
    "# # df_1 = df.dropna()\n",
    "df.to_csv('2023_trend_topic.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_chula_datasci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
